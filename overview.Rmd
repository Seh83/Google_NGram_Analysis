---
title: "Ngram Example"
author: "Anthony Contoleon"
date: "Thursday, August 27, 2015"
output: html_document
---

```{r, echo=FALSE,message=FALSE,warning=FALSE}
## Prototype AdWords Search Term phrase part analysis 1.00
## Importing AdWords CSV.
## Processing for sharing.

library(data.table)
library(reshape2)
library(plyr)
library(dplyr)

library(ngram)
library(ggplot2)

library(AppliedPredictiveModeling)
library(caret)

library(knitr)

options(scipen=1, digits=2)

## PROCESS SUMMARY
# Download AdWords Report.
#   Columns: "Account", "Device", "Network..with.search.partners.", "Search.term", "Match.type", "Clicks", "Impressions", "Cost", "Avg..position", "Added.Excluded", "Converted.clicks", "Campaign", "Ad.group", "Keyword"
# Process/clean AdWords search term report:
#   Open Windows Powershell. Navigate to the appropriate directory.
#   Use following command to convert the file to ASCII to deal with unicode characters:
#   Get-Content [old file]|Set-Content [new file] -encoding ASCII
# Change settings and file references below.

## General settings.
workingDirectory <- "C://Users//Anthony//Documents//GitHub//phrasePartAnalysis" # Place all data files here #
adwordsFile <- "example_set.csv" # Processed search term report
labelsFile <- "label_file.csv" # CSV file. Column heads: Campaign, Labels.
dateString <- format(Sys.time(), "%Y%m%d")

## General Comments on data:
# The numbers provided by AdWords need be be processed to coerce to int/number, due to four digital and up numbers exported with ','
# Dates need to be processed to change from chr to date

## Setting up the working directory for all data file references.
# Ensure all data files for use are in the working directory.
setwd(workingDirectory)
work_dir <- getwd()

## Function for exporting tables to CSVs. 
##Directory path other than work directory and extension needs to be defined 'x', 'y' is the table to be exported.
file_output <- function (x, y){
  path <- paste(work_dir, x, sep ="")
  write.table(y, file = path, sep = ",", row.names = FALSE)
}

## Function import Adwords file. Define file name and remove the 'Total' row.
## Non-latin characters. This was dealt with through converting to ASCII via powershell.
## Get-Content [old file]|Set-Content [new file] -encoding ASCII
adwords_import <- function(x) {
  wip <- read.csv(x, as.is=TRUE, sep="\t", quote="", skip=5, fill=TRUE, flush=TRUE)
  wip <- subset(wip, wip$Account !="Total")
  return(wip)
}

## Function to clean up columns imported as strings due to adwords/MS csv ',' weirdness
chr_number <- function(x, y) {
  x[,y] <- as.numeric(gsub("[^0-9\\.]","", x[,y]))
}

## This is formated for testing purposes, using static file reference.
searchTerm.work_file <- adwords_import(adwordsFile)

## Convert column to numeric.
searchTerm.work_file[, "Cost"] <- chr_number(searchTerm.work_file, "Cost")

## AdWords: need to add labels based on a campaign name match as per external array.
## Loading label array and using the native match function.
adwords.labels <- read.csv(labelsFile, header = TRUE, as.is=TRUE, sep=",", quote="\"")
searchTerm.work_file$Labels <- adwords.labels$Labels[match(searchTerm.work_file$Campaign,adwords.labels$Campaign)]

## Create a subset to work with.
searchTerm.work_file <- searchTerm.work_file[sample(nrow(searchTerm.work_file),size = 5000, replace = TRUE),]

## Creating the ngram objects and data frames from the 
termVector <- toString(searchTerm.work_file[,'Search.term'])
twoGram <- ngram(termVector)
threeGram <- ngram(termVector, n = 3)

## Create the ngram data frames.
ngram2 <- data.frame(V1 = unique(gsub(",", "", get.ngrams(twoGram))))
ngram3 <- data.frame(V1 = unique(gsub(",", "", get.ngrams(threeGram))))

## Remove rows with ?? characters.
ngram2 <- data.frame(V1 = ngram2[!(ngram2$V1=="??"),])
ngram3 <- data.frame(V1 = ngram3[!(ngram3$V1=="??"),])

## Replace numeric values with '##'.
ngram2$V1 <- gsub("[0-9]+", "##", ngram2$V1)
ngram3$V1 <- gsub("[0-9]+", "##", ngram3$V1)

## Clean up duplicate values after numerics have been changed.
ngram2 <- unique(ngram2)
ngram3 <- unique(ngram3)

## Convert data frame to data table
searchTerm.work_file <- data.table(searchTerm.work_file)
labelNgrams.work_file2 <- data.frame()

## Loops, because I just plain hate myself.
for(i in ngram2$V1){
  tryCatch({
    wip <- aggregate(cbind(Impressions, Clicks, Cost, Converted.clicks) ~ Labels + Campaign + Keyword + Search.term, data = searchTerm.work_file[Search.term  %like%  paste('^',i,'$', sep = "") | Search.term  %like%  paste('^',i,'\\s', sep = "") | Search.term  %like%  paste('\\s',i,'$', sep = "") | Search.term  %like%  paste('\\s',i,'\\s', sep = "")], sum)
    wip[, "ngram"] <- i
    labelNgrams.work_file2 <- rbind(labelNgrams.work_file2, wip)
  }, error = function(e){})
}

## Add performance columns
labelNgrams.work_file2$ctr <- labelNgrams.work_file2$Clicks/labelNgrams.work_file2$Impressions
labelNgrams.work_file2$cpc <- labelNgrams.work_file2$Cost/labelNgrams.work_file2$Clicks
labelNgrams.work_file2$cpa <- labelNgrams.work_file2$Cost/labelNgrams.work_file2$Converted.clicks
labelNgrams.work_file2$cvr <- labelNgrams.work_file2$Converted.clicks/labelNgrams.work_file2$Clicks

## Sort out inf values for processing.
labelNgrams.work_file2$cpa[is.infinite(labelNgrams.work_file2$cpa)] <- NA 

## Data Export
file_output(paste0("//ngrams_",dateString, "_2word.csv"), labelNgrams.work_file2)

labelNgrams.work_file3 <- data.frame()

## Loops, because I just plain hate myself.
for(i in ngram3$V1){
  tryCatch({
    wip <- aggregate(cbind(Impressions, Clicks, Cost, Converted.clicks) ~ Labels + Campaign + Keyword + Search.term, data = searchTerm.work_file[Search.term  %like%  paste('^',i,'$', sep = "") | Search.term  %like%  paste('^',i,'\\s', sep = "") | Search.term  %like%  paste('\\s',i,'$', sep = "") | Search.term  %like%  paste('\\s',i,'\\s', sep = "")], sum)
    wip[, "ngram"] <- i
    labelNgrams.work_file3 <- rbind(labelNgrams.work_file3, wip)
  }, error = function(e){})
}

## Add performance columns
labelNgrams.work_file3$ctr <- labelNgrams.work_file3$Clicks/labelNgrams.work_file3$Impressions
labelNgrams.work_file3$cpc <- labelNgrams.work_file3$Cost/labelNgrams.work_file3$Clicks
labelNgrams.work_file3$cpa <- labelNgrams.work_file3$Cost/labelNgrams.work_file3$Converted.clicks
labelNgrams.work_file3$cvr <- labelNgrams.work_file3$Converted.clicks/labelNgrams.work_file3$Clicks

## Sort out inf values for processing.
labelNgrams.work_file3$cpa[is.infinite(labelNgrams.work_file3$cpa)] <- NA 

## Data Export
file_output(paste0("//ngrams_",dateString, "_3word.csv"), labelNgrams.work_file3)

## ngram tables by labels for account sections.

## Processing, analysis and visualisation.
summary2Gram <- aggregate(cbind(Cost, Clicks) ~ ngram + Labels, data = labelNgrams.work_file2, sum)
summary2Gram$cpc <- summary2Gram$Cost/summary2Gram$Clicks
summary2Gram$cpc[is.infinite(summary2Gram$cpc)] <- NA 

## Basic display of clicks across labels per ngram.
summary2Dcast <- dcast(summary2Gram, ngram ~ Labels, value.var = 'Clicks', fun.aggregate = sum)
summary2Dcast$total <- rowSums(summary2Dcast[, c(2:3)])
summary2Dcast <- arrange(summary2Dcast, desc(total))

## Same again for the 3 gram data
summary3Gram <- aggregate(cbind(Cost, Clicks) ~ ngram + Labels, data = labelNgrams.work_file3, sum)
summary3Gram$cpc <- summary3Gram$Cost/summary3Gram$Clicks
summary3Gram$cpc[is.infinite(summary3Gram$cpc)] <- NA 

## Basic display of clicks across labels per ngram.
summary3Dcast <- dcast(summary3Gram, ngram ~ Labels, value.var = 'Clicks', fun.aggregate = sum)
summary3Dcast$total <- rowSums(summary3Dcast[, c(2:3)])
summary3Dcast <- arrange(summary3Dcast, desc(total))

## Graph Time
# A number of visualisations for examining distribution and characteristics of phrase parts in the account.

## Display distribution of clicks by ngrams.
# Create the data set for the graph.
summary2Graph <- aggregate(cbind(Impressions, Cost, Clicks, Converted.clicks) ~ ngram, data = labelNgrams.work_file2, sum)

## Set as data table.
summary2Graph <- data.table(summary2Graph)
#summary2Graph <- summary2Graph[Clicks > 30]

## Change the order of the rows to descending by clicks. Perform a log transformation and a normalisation on Clicks.
summary2Graph <- summary2Graph[order(-Clicks),]
summary2Graph$logClick <- log(summary2Graph$Clicks)
summary2Graph$sdClick <- (summary2Graph$logClick - mean(summary2Graph$logClick))/sd(summary2Graph$logClick)

## Adding a number column to the data table for each row.
summary2Graph <- summary2Graph[, Ngram.number := 1:.N]

## Create data set for box plots showing distribution of CVR by campaign and labels.
graphSet01.S2 <- aggregate(cbind(Impressions, Cost, Clicks, Converted.clicks) ~ ngram + Campaign + Labels, data = labelNgrams.work_file2, sum)
graphSet01.S2$logClicks <- log(graphSet01.S2$Clicks)
graphSet01.S2$logCost <- log(graphSet01.S2$Cost)

## Setting the data frame as a data table.
graphSet01.S2 <- data.table(graphSet01.S2)

## Creating and cleaning the CVR column.
graphSet01.S2$cvr <- graphSet01.S2$Converted.clicks/graphSet01.S2$Clicks
graphSet01.S2$cvr[is.infinite(graphSet01.S2$cvr)] <- NA
graphSet01.S2$cpc <- graphSet01.S2$Cost/graphSet01.S2$Clicks
graphSet01.S2$cpc[is.infinite(graphSet01.S2$cpc)] <- NA
graphSet01.S2$cpa <- graphSet01.S2$Cost/graphSet01.S2$Converted.clicks
graphSet01.S2$cpa[is.infinite(graphSet01.S2$cpa)] <- NA

## Create log values for graphing.
graphSet01.S2$logClicks <- log(graphSet01.S2$Clicks)
graphSet01.S2$logCost <- log(graphSet01.S2$Cost)
graphSet01.S2$sqrtCost <- sqrt(graphSet01.S2$Cost)
graphSet01.S2$sdCost <- sd(graphSet01.S2$Cost)

## Setting the data frame as a data table.
graphSet01.S2 <- data.table(graphSet01.S2)

## Create the cut down data set for the confidence interval graph.
confSet <- graphSet01.S2[ Clicks > 10 & Converted.clicks > 0 ,c(1:3, 6:7, 10), with = FALSE]

## Create the grouping for the summarise function.
grouped <- group_by(confSet, Campaign, Labels)

## Create a data set including summary statistics for differences from means.
confSum.df <- summarise(grouped, mcvr = sum(Converted.clicks)/sum(Clicks), n = length(cvr), mean = mean(cvr), sd = sd(cvr), se = sd(cvr) / sqrt(length(cvr)), ci = (sd(cvr) / sqrt(length(cvr))) * (qt(0.95/2 + .5, length(cvr)-1)))

## Means for labels.
grouped.2 <- group_by(searchTerm.work_file, Labels)
confSum2.df <- summarise(grouped.2, cvr = sum(Converted.clicks)/sum(Clicks), n = length(Search.term))
confSum2.df <- data.table(confSum2.df)

```

#Introduction

This is a quick example of the kind of output being produced from the <a href="https://github.com/anthonypc/phrasePartAnalysis">https://github.com/anthonypc/phrasePartAnalysis</a> project as per the blog post:.

The inital analysis of the file and the creation of the ngram lists are all dealt with in the code. Other than R, the entire process should not need any other tools. For ease of processing, I would recommend following the instructions on using Powershell to take care of non-latin characters to avoid issues with encoding.

#Output

For the most part the tables produced from line 145 are interesting for identifying ngrams shared across the account bwith total of clicks. The tables below and others like them with additional statistics would meet this need.

```{r}
kable(head(summary2Dcast), digits=2)
kable(head(summary3Dcast), digits=2)

kable(head(arrange(summary2Gram, -desc(ngram))), digits=2)
kable(head(arrange(summary3Gram, -desc(ngram))), digits=2)

```

#Plots

The graphs are fairly straight forward, though the initial distribution did require some work on the data set to produce the chart as per below.

```{r, echo=FALSE}
ggplot(summary2Graph, aes(x = Ngram.number, y = logClick)) + geom_area(fill= "black", alpha = .2) + geom_line() + ggtitle("Example Distribution of Clicks by Search Terms [unfiltered]")
```

A simple histogram is easy to produce. This example also includes a density plot.

```{r, echo=FALSE}
ggplot(graphSet01.S2, aes(x = Clicks, alpha = .5)) + geom_histogram() + ggtitle("Example Clicks Histogram") + facet_wrap( ~ Labels, ncol = 2)

ggplot(graphSet01.S2, aes(x = Clicks, alpha = .5)) + geom_density() + ggtitle("Example Clicks Histogram") + facet_wrap( ~ Labels, ncol = 2)
```

Though for paid search I find that scatter plots are very useful, as generally I am looking for bivariate outliers rather than univariate .

```{r, echo=FALSE}
ggplot(graphSet01.S2, aes(x = logCost, y = logClicks)) + geom_point() + ggtitle("Example Clicks to Cost Scatter Plot") + facet_wrap( ~ Labels, ncol = 2)
```

The following is an example that manipulates the labels on the points to highlight those of interest, and leave the plot 'fairly' uncluttered.

```{r, echo=FALSE}
ggplot(graphSet01.S2[Clicks > 10], aes(x = Clicks, y = cvr, size = sqrtCost)) + geom_point() + facet_wrap( ~ Labels, ncol = 2) + geom_text(aes(label = ifelse((Cost-mean(Cost))/sdCost > 3, ngram, "")), hjust = 1, vjust = 1) + ggtitle("Example CVR scatterplot by Labels [unfiltered]")
```

The following table is a simple one looking at the distribution of the CVR per ngram within groups of labels and campaigns. The data is not really appropriate for this, but it is an interesting exercise in looking how the number of observations can affect the error of the mean CVR.

```{r, echo=FALSE}
kable(head(confSum.df), digits=2)
```

Which is then plotted here, where the line through each plot is the group mean.

```{r, echo=FALSE,warning=FALSE}
ggplot(confSum.df, aes(x = Campaign, y = mcvr, group = 1)) + geom_errorbar(width=.1, aes(ymin = mcvr - ci, ymax = mcvr + ci)) + geom_point(shape = 21, size = 3, fill = "white") + facet_wrap( ~ Labels, ncol = 2) + geom_hline(data = confSum2.df, aes(yintercept = cvr)) + theme(axis.text.x = element_text(angle = 90, hjust = 1)) + scale_x_discrete(labels = abbreviate)
```

For context here is a plot of clicks to CVR.

```{r, echo=FALSE,warning=FALSE}
ggplot(graphSet01.S2, aes(x = Clicks, y = cvr)) + geom_point() + ggtitle("Example Clicks to CVR Scatter Plot") + facet_wrap( ~ Labels, ncol = 2)
```

#Extreme Values

There are a few techniques used for testing assumptions for multivariate regression, <a href="https://en.wikipedia.org/wiki/Mahalanobis_distance">Mahalanobis Distance</a> and <a href="https://en.wikipedia.org/wiki/Cook%27s_distance">Cook's Distance</a>. The data used here is certainly not appropriate for regression, the two tests mentioned above can be used to identify points that do not exhibit the same relationship between Clicks and Conversions.

```{r, echo=FALSE,message=FALSE,warning=FALSE}
## Assessing for the outliers.

# Analysis packages
library(car)
library(gvlma)
library(MASS)
library(QuantPsyc)
library(Hmisc)
library(corrplot)

## Function for exporting tables to CSVs. 
##Directory path other than work directory and extension needs to be defined 'x', 'y' is the table to be exported.
file_output <- function (x, y){
  path <- paste(work_dir, x, sep ="")
  write.table(y, file = path, sep = ",", row.names = FALSE)
}

## This is going to be based on a simple general linear model
is.nan.data.frame <- function(x)
  do.call(cbind, lapply(x, is.nan))

fitSet02.df<- aggregate(cbind(Impressions, Cost, Clicks, Converted.clicks) ~ ngram + Campaign + Labels, data = labelNgrams.work_file2, sum)
fitSet03.df<- aggregate(cbind(Impressions, Cost, Clicks, Converted.clicks) ~ ngram + Campaign + Labels, data = labelNgrams.work_file2, sum)

fitSet02.df$cvr <- fitSet02.df$Converted.clicks/fitSet02.df$Clicks
fitSet03.df$cvr <- fitSet03.df$Converted.clicks/fitSet03.df$Clicks

fitSet02.df[is.nan(fitSet02.df)] <- 0
fitSet03.df[is.nan(fitSet03.df)] <- 0

fitSet02.df$cvr[is.infinite(fitSet02.df$cvr)] <- 0
fitSet03.df$cvr[is.infinite(fitSet03.df$cvr)] <- 0

fitSet02.df[is.na(fitSet02.df)] <- 0
fitSet03.df[is.na(fitSet03.df)] <- 0

## Analysing one set of data as per the new sets created above.
## This is using the two word gram data frame as per above.
fitSet02.df$Campaign <- as.factor(fitSet02.df$Campaign)
fitSet02.dt <- data.table(fitSet02.df)
fit <- lm(Converted.clicks ~ Campaign * Clicks, data = fitSet02.dt, weight = Cost)
```

The following is a simple check for outliers against the fitted linear model with conversions as an outcome by clicks against campaigns and weighted on cost.

```{r, echo=FALSE}
# Assessing Outliers
outlierTest(fit) # Bonferonni p-value for most extreme obs
rows01 <- c(126, 161, 123, 138, 74)
data01 <- fitSet02.dt[c(126, 161, 123, 138, 74),]
data01$Observation <- rows01
kable(data01, digits=2)
```

And here is a quick review of the top ten search terms containing observation 123.

```{r, echo=FALSE}
labelNgrams.work_file2 <- data.table(labelNgrams.work_file2)
kable(head(arrange(labelNgrams.work_file2[ngram %like% "name word" | ngram %like% "next other" | ngram %like% "word uno" | ngram %like% "test string" | ngram %like% "text word", c(3:8,13), with = FALSE], desc(Clicks)), 10), digits=2)
```

Followed by a quick analysis of how this ngram's clicks are distributed across different campaigns.

```{r, echo=FALSE}
summary2Camp <- aggregate(cbind(Converted.clicks, Clicks) ~ ngram + Campaign, data = labelNgrams.work_file2[ngram %like% "name word" | ngram %like% "next other" | ngram %like% "word uno" | ngram %like% "test string" | ngram %like% "text word"], sum)

summary2Camp$cvr <- summary2Camp$Converted.clicks/summary2Camp$Clicks
summary2Camp$cvr[is.infinite(summary2Camp$cvr)] <- NA 

summary2DCam <- dcast(summary2Camp, ngram ~ Campaign, value.var = 'Clicks', fun.aggregate = sum)
kable(summary2DCam, digits=2)

```

The next is a look at the conversion rate by campaign.

```{r, echo=FALSE}
summary2DCav <- dcast(summary2Camp, ngram ~ Campaign, value.var = 'cvr')
kable(summary2DCav, digits=2)
```

Now the following is a closer look at a number of other different techniques for identifying points which are sigificantly different from the bulk of the account. The next is a look at the rows returned by Cook's Distance.

```{r, echo=FALSE}
# Cook's D plot
# identify D values > 4/(n-k-1) 
cutoff <- 4/((nrow(fitSet02.df)-length(fit$coefficients)-2)) 
plot(fit, which = 4, cook.levels = cutoff)

rows02 <- c(28, 74, 123)
data02 <- fitSet02.dt[c(28, 74, 123),]
data02$Observation <- rows02
kable(data02, digits=2)
```

Next is a plot of residuals against the model where the Cook's distance of each point is displayed as the size of the circle for each point.

```{r, echo=FALSE}
# Influence Plot 
influencePlot(fit, id.method = "note.worthy", main = "Influence Plot", sub = "Circle size is proportial to Cook's Distance", id.cex = 1, id.n = 2)

rows03 <- c(28, 74, 96, 123)
data03 <- fitSet02.dt[c(28, 74, 96, 123),]
data03$Observation <- rows03
kable(data03, digits=2)
```

The following are a series of graphs and tables of the relationship between clicks and converted clicks. Both Mahalanobis Disance and Cook's Distance are included in the tables for each campaign.


```{r, echo=FALSE,message=FALSE,warning=FALSE}
## Produce a table of extreme rows using Mahalanobis Disance and Cook's D.
for(i in c("Campaign 01", "Campaign 02", "Campaign 03", "Campaign 04")) {
## Calculate overall Mahalonbis Distance for clicks and conversion numbers to identify outlying values.
  sx <- cov(fitSet02.dt[Campaign == i,6:7, with = FALSE])
  m1 <- mahalanobis(fitSet02.dt[Campaign == i,6:7, with = FALSE], colMeans(fitSet02.dt[Campaign == i, 6:7, with = FALSE]), sx)

## Caculate and add Cook's Distance as per the fited model.
  d1 <- cooks.distance(fit)
  wip <- cbind(fitSet02.dt[Campaign == i,], m1, d1)

## Print the rows reaching a certain threshold.
  wip <- na.omit(wip[d1 > 4/nrow(wip), ])

plot <- ggplot(wip, aes(x = Clicks, y = Converted.clicks)) + geom_point() + geom_text(aes(label = ifelse((d1 > 4/nrow(wip)), ngram, "")), hjust = 1, vjust = 1) + ggtitle(paste("Clicks to Converted Clicks for ",i," with Cook's Distance", sep = ""))

print(plot)

print(arrange(wip, desc(d1)))
  
}

```