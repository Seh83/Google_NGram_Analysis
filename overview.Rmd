---
title: "Ngram Example"
author: "Anthony Contoleon"
date: "Thursday, August 27, 2015"
output: html_document
---

```{r, echo=FALSE,message=FALSE,warning=FALSE}
## Prototype AdWords Search Term phrase part analysis 1.00
## Importing AdWords CSV.
## Processing for sharing.

library(data.table)
library(reshape2)
library(plyr)
library(dplyr)

library(ngram)
library(ggplot2)

library(AppliedPredictiveModeling)
library(caret)

library(knitr)

## PROCESS SUMMARY
# Download AdWords Report.
#   Columns: "Account", "Device", "Network..with.search.partners.", "Search.term", "Match.type", "Clicks", "Impressions", "Cost", "Avg..position", "Added.Excluded", "Converted.clicks", "Campaign", "Ad.group", "Keyword"
# Process/clean AdWords search term report:
#   Open Windows Powershell. Navigate to the appropriate directory.
#   Use following command to convert the file to ASCII to deal with unicode characters:
#   Get-Content [old file]|Set-Content [new file] -encoding ASCII
# Change settings and file references below.

## General settings.
adwordsFile <- "example_set.csv" # Processed search term report
labelsFile <- "label_file.csv" # CSV file. Column heads: Campaign, Labels.
dateString <- format(Sys.time(), "%Y%m%d")

## General Comments on data:
# The numbers provided by AdWords need be be processed to coerce to int/number, due to four digital and up numbers exported with ','
# Dates need to be processed to change from chr to date

## Function import Adwords file. Define file name and remove the 'Total' row.
## Non-latin characters. This was dealt with through converting to ASCII via powershell.
## Get-Content [old file]|Set-Content [new file] -encoding ASCII
adwords_import <- function(x) {
  wip <- read.csv(x, as.is=TRUE, sep="\t", quote="", skip=5, fill=TRUE, flush=TRUE)
  wip <- subset(wip, wip$Account !="Total")
  return(wip)
}

## Function to clean up columns imported as strings due to adwords/MS csv ',' weirdness
chr_number <- function(x, y) {
  x[,y] <- as.numeric(gsub("[^0-9\\.]","", x[,y]))
}

## This is formated for testing purposes, using static file reference.
searchTerm.work_file <- adwords_import(adwordsFile)

## Convert column to numeric.
searchTerm.work_file[, "Cost"] <- chr_number(searchTerm.work_file, "Cost")

## AdWords: need to add labels based on a campaign name match as per external array.
## Loading label array and using the native match function.
adwords.labels <- read.csv(labelsFile, header = TRUE, as.is=TRUE, sep=",", quote="\"")
searchTerm.work_file$Labels <- adwords.labels$Labels[match(searchTerm.work_file$Campaign,adwords.labels$Campaign)]

## Create a subset to work with.
searchTerm.work_file <- searchTerm.work_file[sample(nrow(searchTerm.work_file),size = 5000, replace = TRUE),]

## Creating the ngram objects and data frames from the 
termVector <- toString(searchTerm.work_file[,'Search.term'])
twoGram <- ngram(termVector)
threeGram <- ngram(termVector, n = 3)

## Create the ngram data frames.
ngram2 <- data.frame(V1 = unique(gsub(",", "", get.ngrams(twoGram))))
ngram3 <- data.frame(V1 = unique(gsub(",", "", get.ngrams(threeGram))))

## Remove rows with ?? characters.
ngram2 <- data.frame(V1 = ngram2[!(ngram2$V1=="??"),])
ngram3 <- data.frame(V1 = ngram3[!(ngram3$V1=="??"),])

## Replace numeric values with '##'.
ngram2$V1 <- gsub("[0-9]+", "##", ngram2$V1)
ngram3$V1 <- gsub("[0-9]+", "##", ngram3$V1)

## Clean up duplicate values after numerics have been changed.
ngram2 <- unique(ngram2)
ngram3 <- unique(ngram3)

## Convert data frame to data table
searchTerm.work_file <- data.table(searchTerm.work_file)
labelNgrams.work_file2 <- data.frame()
```

```{r, echo=FALSE,message=FALSE,warning=FALSE}

## Loops, because I just plain hate myself.
for(i in ngram2$V1){
  tryCatch({
    wip <- aggregate(cbind(Impressions, Clicks, Cost, Converted.clicks) ~ Labels + Campaign + Keyword + Search.term, data = searchTerm.work_file[Search.term  %like%  paste('^',i,'$', sep = "") | Search.term  %like%  paste('^',i,'\\s', sep = "") | Search.term  %like%  paste('\\s',i,'$', sep = "") | Search.term  %like%  paste('\\s',i,'\\s', sep = "")], sum)
    wip[, "ngram"] <- i
    labelNgrams.work_file2 <- rbind(labelNgrams.work_file2, wip)
  }, error = function(e){})
}

## Add performance columns
labelNgrams.work_file2$ctr <- labelNgrams.work_file2$Clicks/labelNgrams.work_file2$Impressions
labelNgrams.work_file2$cpc <- labelNgrams.work_file2$Cost/labelNgrams.work_file2$Clicks
labelNgrams.work_file2$cpa <- labelNgrams.work_file2$Cost/labelNgrams.work_file2$Converted.clicks
labelNgrams.work_file2$cvr <- labelNgrams.work_file2$Converted.clicks/labelNgrams.work_file2$Clicks

## Sort out inf values for processing.
labelNgrams.work_file2$cpa[is.infinite(labelNgrams.work_file2$cpa)] <- NA 

labelNgrams.work_file3 <- data.frame()

## Loops, because I just plain hate myself.
for(i in ngram3$V1){
  tryCatch({
    wip <- aggregate(cbind(Impressions, Clicks, Cost, Converted.clicks) ~ Labels + Campaign + Keyword + Search.term, data = searchTerm.work_file[Search.term  %like%  paste('^',i,'$', sep = "") | Search.term  %like%  paste('^',i,'\\s', sep = "") | Search.term  %like%  paste('\\s',i,'$', sep = "") | Search.term  %like%  paste('\\s',i,'\\s', sep = "")], sum)
    wip[, "ngram"] <- i
    labelNgrams.work_file3 <- rbind(labelNgrams.work_file3, wip)
  }, error = function(e){})
}

## Add performance columns
labelNgrams.work_file3$ctr <- labelNgrams.work_file3$Clicks/labelNgrams.work_file3$Impressions
labelNgrams.work_file3$cpc <- labelNgrams.work_file3$Cost/labelNgrams.work_file3$Clicks
labelNgrams.work_file3$cpa <- labelNgrams.work_file3$Cost/labelNgrams.work_file3$Converted.clicks
labelNgrams.work_file3$cvr <- labelNgrams.work_file3$Converted.clicks/labelNgrams.work_file3$Clicks

## Sort out inf values for processing.
labelNgrams.work_file3$cpa[is.infinite(labelNgrams.work_file3$cpa)] <- NA 
```

```{r, echo=FALSE,message=FALSE,warning=FALSE}

## ngram tables by labels for account sections.

## Processing, analysis and visualisation.
summary2Gram <- aggregate(cbind(Cost, Clicks, Converted.clicks) ~ ngram + Labels, data = labelNgrams.work_file2, sum)
summary2Gram$cpc <- summary2Gram$Cost/summary2Gram$Clicks
summary2Gram$cpc[is.infinite(summary2Gram$cpc)] <- NA 
summary2Gram$cvr <- summary2Gram$Clicks/summary2Gram$Converted.clicks
summary2Gram$cvr[is.infinite(summary2Gram$cvr)] <- NA 

## Basic display of clicks across labels per ngram.
summary2Dcast <- dcast(summary2Gram, ngram ~ Labels, value.var = 'Clicks', fun.aggregate = sum)
summary2Dcast$total <- rowSums(summary2Dcast[, c(2:3)])
summary2Dcast <- arrange(summary2Dcast, desc(total))

## Basic display of clicks across labels per ngram.
summary2Dcasta <- dcast(summary2Gram, ngram ~ Labels, value.var = 'cvr', fun.aggregate = max)

## Same again for the 3 gram data
summary3Gram <- aggregate(cbind(Cost, Clicks, Converted.clicks) ~ ngram + Labels, data = labelNgrams.work_file3, sum)
summary3Gram$cpc <- summary3Gram$Cost/summary3Gram$Clicks
summary3Gram$cpc[is.infinite(summary3Gram$cpc)] <- NA 
summary3Gram$cvr <- summary3Gram$Clicks/summary3Gram$Converted.clicks
summary3Gram$cvr[is.infinite(summary3Gram$cvr)] <- NA 

## Basic display of clicks across labels per ngram.
summary3Dcast <- dcast(summary3Gram, ngram ~ Labels, value.var = 'Clicks', fun.aggregate = sum)
summary3Dcast$total <- rowSums(summary3Dcast[, c(2:3)])
summary3Dcast <- arrange(summary3Dcast, desc(total))

## Basic display of clicks across labels per ngram.
summary3Dcasta <- dcast(summary3Gram, ngram ~ Labels, value.var = 'cvr', fun.aggregate = max)
```

```{r, echo=FALSE,message=FALSE,warning=FALSE}

## Graph Time
# A number of visualisations for examining distribution and characteristics of phrase parts in the account.

## Display distribution of clicks by ngrams.
# Create the data set for the graph.
summary2Graph <- aggregate(cbind(Impressions, Cost, Clicks, Converted.clicks) ~ ngram, data = labelNgrams.work_file2, sum)

## Set as data table.
summary2Graph <- data.table(summary2Graph)
#summary2Graph <- summary2Graph[Clicks > 30]

## Change the order of the rows to descending by clicks. Perform a log transformation and a normalisation on Clicks.
summary2Graph <- summary2Graph[order(-Clicks),]
summary2Graph$logClick <- log(summary2Graph$Clicks)
summary2Graph$sdClick <- (summary2Graph$logClick - mean(summary2Graph$logClick))/sd(summary2Graph$logClick)

## Adding a number column to the data table for each row.
summary2Graph <- summary2Graph[, Ngram.number := 1:.N]

## Create data set for box plots showing distribution of CVR by campaign and labels.
graphSet01.S2 <- aggregate(cbind(Impressions, Cost, Clicks, Converted.clicks) ~ ngram + Campaign + Labels, data = labelNgrams.work_file2, sum)
graphSet01.S2$logClicks <- log(graphSet01.S2$Clicks)
graphSet01.S2$logCost <- log(graphSet01.S2$Cost)

## Setting the data frame as a data table.
graphSet01.S2 <- data.table(graphSet01.S2)

## Creating and cleaning the CVR column.
graphSet01.S2$cvr <- graphSet01.S2$Converted.clicks/graphSet01.S2$Clicks
graphSet01.S2$cvr[is.infinite(graphSet01.S2$cvr)] <- NA
graphSet01.S2$cpc <- graphSet01.S2$Cost/graphSet01.S2$Clicks
graphSet01.S2$cpc[is.infinite(graphSet01.S2$cpc)] <- NA
graphSet01.S2$cpa <- graphSet01.S2$Cost/graphSet01.S2$Converted.clicks
graphSet01.S2$cpa[is.infinite(graphSet01.S2$cpa)] <- NA

## Create log values for graphing.
graphSet01.S2$logClicks <- log(graphSet01.S2$Clicks)
graphSet01.S2$logCost <- log(graphSet01.S2$Cost)
graphSet01.S2$sqrtCost <- sqrt(graphSet01.S2$Cost)
graphSet01.S2$sdCost <- sd(graphSet01.S2$Cost)

## Setting the data frame as a data table.
graphSet01.S2 <- data.table(graphSet01.S2)

## Create the cut down data set for the confidence interval graph.
confSet <- graphSet01.S2[ Clicks > 10 & Converted.clicks > 0 ,c(1:3, 6:7, 10), with = FALSE]

## Create the grouping for the summarise function.
grouped <- group_by(confSet, Campaign, Labels)

## Create a data set including summary statistics for differences from means.
confSum.df <- summarise(grouped, mcvr = sum(Converted.clicks)/sum(Clicks), n = length(cvr), mean = mean(cvr), sd = sd(cvr), se = sd(cvr) / sqrt(length(cvr)), ci = (sd(cvr) / sqrt(length(cvr))) * (qt(0.95/2 + .5, length(cvr)-1)))

## Means for labels.
grouped.2 <- group_by(searchTerm.work_file, Labels)
confSum2.df <- summarise(grouped.2, cvr = sum(Converted.clicks)/sum(Clicks), n = length(Search.term))
confSum2.df <- data.table(confSum2.df)

```

#Introduction

This is a quick example of the kind of output being produced from the <a href="https://github.com/anthonypc/phrasePartAnalysis">https://github.com/anthonypc/phrasePartAnalysis</a> project as per the blog post: <a href="http://contoleon.com/blog/2015/08/30/text-processing-n-grams-and-paid-search/">Text Processing, N-Grams And Paid Search</a>.

The inital analysis of the file and the creation of the ngram lists are all dealt with in the code. Other than R, the entire process should not need any other tools. For ease of processing, I would recommend following the instructions on using Powershell to take care of non-latin characters to avoid issues with encoding.

Please read the read me in GitHub and the comments in the code itself for an explaination on how it works, what the inputs need to be and an overview of the process.

#Summary

This process identifies two and three word combinations in search terms and assigned performance statistics against these. The objective is to identify phrase parts within the account, some of which will be shared between campaigns and adgroups, and use the information to shape keyword strategy.

#Output

There are two files as per the readme used to produce the output for this file. Most of the output is produced by the ngram-outlier-influence-0-01.R script, though some of the example output from ngrams-ext-1-00.R is included.

##ngrams-ext-1-00.R Output

The tables produced from line 145 in the main R script (ngrams-ext-1-00.R) are useful for identifying ngrams shared across the account. The tables below and others like them with additional statistics would meet this need.

The volume of clicks between the labels in the example set are not even for the most part.

```{r}
kable(head(summary2Dcast), digits=2)
kable(head(summary3Dcast), digits=2)
```

The following are displaying conversion rate by ngram and label.

```{r}
kable(head(summary3Dcasta), digits=2)
kable(head(summary3Dcasta), digits=2)
```

The following tables are the main CSV output, with a file produced against two and three word phrase part combinations.

```{r}
kable(head(arrange(labelNgrams.work_file2, desc(Clicks))), digits=2)
kable(head(arrange(labelNgrams.work_file3, desc(Clicks))), digits=2)

```

#Plots

The graphs are fairly straight forward, though the initial distribution did require some work on the data set to produce the chart as per below.

```{r, echo=FALSE}
ggplot(summary2Graph, aes(x = Ngram.number, y = logClick)) + geom_area(fill= "black", alpha = .2) + geom_line() + ggtitle("Example Distribution of Clicks by Search Terms [unfiltered]")
```

A histogram for clicks within the account by ngram demonstrates the distribution of volume. This example also includes a density plot.

```{r, echo=FALSE}
ggplot(graphSet01.S2, aes(x = Clicks, alpha = .5)) + geom_histogram() + ggtitle("Example Clicks Histogram") + facet_wrap( ~ Labels, ncol = 2)

ggplot(graphSet01.S2, aes(x = Clicks, alpha = .5)) + geom_density() + ggtitle("Example Clicks Histogram") + facet_wrap( ~ Labels, ncol = 2)
```

Univariate distributions are useful. With a lot of paid search analysis scatter plots are very useful as bivariate outliers are often more informative than univariate. Such as data points with extreme cost and volume figures as per this chart. The values are log transformed to produce a slightly more normal distribution.

```{r, echo=FALSE}
ggplot(graphSet01.S2, aes(x = logCost, y = logClicks)) + geom_point() + ggtitle("Example Clicks to Cost Scatter Plot") + facet_wrap( ~ Labels, ncol = 2)
```

The following is an example that manipulates the labels on the points to highlight those of interest, and leave the plot 'fairly' uncluttered.

```{r, echo=FALSE}
ggplot(graphSet01.S2[Clicks > 10], aes(x = log(Clicks), y = log(cvr), size = sqrtCost)) + geom_point() + facet_wrap( ~ Labels, ncol = 2) + geom_text(aes(label = ifelse((Cost-mean(Cost))/sdCost > 3, ngram, "")), hjust = 1, vjust = 1) + ggtitle("Example CVR scatterplot by Labels [unfiltered]")
```

The following table is a simple one looking at the distribution of the CVR per ngram within groups of labels and campaigns. The data is not really appropriate for this, but it is an interesting exercise in looking how the number of observations can affect the error of the mean CVR.

```{r, echo=FALSE}
kable(head(confSum.df), digits=2)
```

This is another average of a CVR plot where the actual CVR for the group (label in this case) is plotted as a line through each facet.

```{r, echo=FALSE,warning=FALSE}
ggplot(confSum.df, aes(x = Campaign, y = mcvr, group = 1)) + geom_errorbar(width=.1, aes(ymin = mcvr - ci, ymax = mcvr + ci)) + geom_point(shape = 21, size = 3, fill = "white") + facet_wrap( ~ Labels, ncol = 2) + geom_hline(data = confSum2.df, aes(yintercept = cvr)) + theme(axis.text.x = element_text(angle = 90, hjust = 1)) + scale_x_discrete(labels = abbreviate)
```

For context here is a plot of clicks to CVR without a transformation applied to the values.

```{r, echo=FALSE,warning=FALSE}
ggplot(graphSet01.S2, aes(x = Clicks, y = cvr)) + geom_point() + ggtitle("Example Clicks to CVR Scatter Plot") + facet_wrap( ~ Labels, ncol = 2)
```

#Extreme Values

##ngram-outlier-influence-0-01.R Output

Ideally keywords grouped together in a paid search account perform more or less the same, where the relationship between what is spent and how much traffic is recieved should be close, and the same for the number of clicks and the number of conversions. 

There are a few techniques used for testing assumptions for multivariate regression. A number of these are used for identifying outliers, those with high leverage and influenctial data points. Both <a href="https://en.wikipedia.org/wiki/Mahalanobis_distance">Mahalanobis Distance</a> and <a href="https://en.wikipedia.org/wiki/Cook%27s_distance">Cook's Distance</a> are used to address these issues. The data used here is certainly not appropriate for regression, the two tests mentioned above can be used to identify points that do not exhibit the same relationship between Clicks and Conversions.

Most distributions of those values would look a little more like this, where overtime there should be a relationship between the two values as optimisation activity in the account stablises performance. Though most likely with a heavier skew towards 0 on either axis. The example set in the repository does not.

```{r, echo=FALSE}
x <- qnorm(runif(100,min=pnorm(0),max=pnorm(20)), mean = 5, sd = 1)
y <- qnorm(runif(100,min=pnorm(0),max=pnorm(20)), mean = 7, sd = 2)

c <- data.frame(x,y)
x_name <- "clicks"
y_name <- "cost"
names(c) <- c(x_name,y_name)
ggplot(c, aes(x = clicks, y = cost)) + geom_point() + ggtitle("Hypothetical Random Normal Distribution")
```


```{r, echo=FALSE,message=FALSE,warning=FALSE}
## Assessing for the outliers.

# Analysis packages
library(car)
library(gvlma)
library(MASS)
library(QuantPsyc)
library(Hmisc)
library(corrplot)

## This is going to be based on a simple general linear model
is.nan.data.frame <- function(x)
  do.call(cbind, lapply(x, is.nan))

fitSet02.df<- aggregate(cbind(Impressions, Cost, Clicks, Converted.clicks) ~ ngram + Campaign + Labels, data = labelNgrams.work_file2, sum)
fitSet03.df<- aggregate(cbind(Impressions, Cost, Clicks, Converted.clicks) ~ ngram + Campaign + Labels, data = labelNgrams.work_file2, sum)

fitSet02.df$cvr <- fitSet02.df$Converted.clicks/fitSet02.df$Clicks
fitSet03.df$cvr <- fitSet03.df$Converted.clicks/fitSet03.df$Clicks

fitSet02.df[is.nan(fitSet02.df)] <- 0
fitSet03.df[is.nan(fitSet03.df)] <- 0

fitSet02.df$cvr[is.infinite(fitSet02.df$cvr)] <- 0
fitSet03.df$cvr[is.infinite(fitSet03.df$cvr)] <- 0

fitSet02.df[is.na(fitSet02.df)] <- 0
fitSet03.df[is.na(fitSet03.df)] <- 0

## Analysing one set of data as per the new sets created above.
## This is using the two word gram data frame as per above.
fitSet02.df$Campaign <- as.factor(fitSet02.df$Campaign)
fitSet02.dt <- data.table(fitSet02.df)
fit <- lm(Converted.clicks ~ Campaign * Clicks, data = fitSet02.dt, weight = Cost)
```

The following is a simple check for outliers against the fitted linear model with conversions as an outcome by clicks against campaigns and weighted on cost.

The outliers reported below where identified by a Bonferroni Outlier Test as per the car pacakge.

```{r, echo=FALSE}
# Assessing Outliers
outlierTest(fit) # Bonferonni p-value for most extreme obs
rows01 <- c(126, 161, 123, 74)
data01 <- fitSet02.dt[c(126, 161, 123, 74),]
data01$Observation <- rows01
kable(data01, digits=2)
```

The first table is the top ten individual observations by clicks matching to the ngrams identified above. Next is a quick review of the performance of the ngrams identified as outliers by campaign.

```{r, echo=FALSE}
labelNgrams.work_file2 <- data.table(labelNgrams.work_file2)

kable(head(arrange(labelNgrams.work_file2[ngram %like% "name word" | ngram %like% "next other" | ngram %like% "word uno" | ngram %like% "test string" | ngram %like% "text word", c(3:8,13), with = FALSE], desc(Clicks)), 10), digits=2)

summary2Camp <- aggregate(cbind(Converted.clicks, Clicks) ~ ngram + Campaign, data = labelNgrams.work_file2[ngram %like% "name word" | ngram %like% "next other" | ngram %like% "word uno" | ngram %like% "test string" | ngram %like% "text word"], sum)
````

The following is a table of clicks by ngram and campaign.

```{r, echo=FALSE}
summary2Camp$cvr <- summary2Camp$Converted.clicks/summary2Camp$Clicks
summary2Camp$cvr[is.infinite(summary2Camp$cvr)] <- NA 

summary2DCam <- dcast(summary2Camp, ngram ~ Campaign, value.var = 'Clicks', fun.aggregate = sum)
kable(summary2DCam, digits=2)
```

The following is a table of conversion rate by ngram and campaign.

```{r, echo=FALSE}
summary2DCav <- dcast(summary2Camp, ngram ~ Campaign, value.var = 'cvr')
kable(summary2DCav, digits=2)
```

# Other Tests

There are a number of different methods included here for identifying extreme points. The next is a look at the rows returned by Cook's Distance. Test identified fewer points than the previous one, and it also identified observation 28, which has an unusually low number of conversions for the clicks it has recieved.

```{r, echo=FALSE}
# Cook's D plot
# identify D values > 4/(n-k-1) 
cutoff <- 4/((nrow(fitSet02.df)-length(fit$coefficients)-2)) 
plot(fit, which = 4, cook.levels = cutoff)

rows02 <- c(28, 74, 123)
data02 <- fitSet02.dt[c(28, 74, 123),]
data02$Observation <- rows02

kable(data02, digits=2)
```

Next is a plot of residuals against the model where the Cook's distance of each point is displayed as the size of the circle for each point. The plot has labeled the most influential points, those which have the most affect on the linear model created at the start of this process.

Again the observations identified were different from the outlier test.

```{r, echo=FALSE}
influencePlot(fit, id.method = "note.worthy", main = "Influence Plot", sub = "Circle size is proportial to Cook's Distance", id.cex = 1, id.n = 6)

rows03 <- c(28, 74, 96, 123)
data03 <- fitSet02.dt[c(28, 74, 96, 123),]
data03$Observation <- rows03

kable(data03, digits=2)
```

# By Campaign

The following are a series of graphs and tables of the relationship between clicks and converted clicks. Both Mahalanobis Disance and Cook's Distance are included in the tables for each campaign.

In the output below the first table for each campaign under the scatterplot is sorted by Cook's Distance and the second Mahalanobis Disance and It is interesting that each table while sharing observations, orders them differently.

## Campaign 01

```{r, echo=FALSE,message=FALSE,warning=FALSE}
## Calculate overall Mahalonbis Distance for clicks and conversion numbers to identify outlying values.
sx1 <- cov(fitSet02.dt[Campaign == "Campaign 01" & Clicks > 0,6:7, with = FALSE])
md1 <- mahalanobis(fitSet02.dt[Campaign == "Campaign 01" & Clicks > 0,6:7, with = FALSE], colMeans(fitSet02.dt[Campaign == "Campaign 01", 6:7, with = FALSE]), sx1)

## Caculate and add Cook's Distance as per the fited model.
cd1 <- cooks.distance(
  lm(Converted.clicks ~ Clicks, data = fitSet02.dt[Campaign == "Campaign 01" & Clicks > 0], weight = Cost)
  )
wip1 <- cbind(fitSet02.dt[Campaign == "Campaign 01" & Clicks > 0,], md1, cd1)

ggplot(wip1, aes(x = log(Clicks), y = log(Converted.clicks))) + geom_point(aes(colour = md1, size = cd1)) + geom_text(aes(label = ifelse((cd1 > 4/nrow(wip1)), ngram, "")), hjust = 1, vjust = 1) + ggtitle(paste("Clicks to Converted Clicks for Campaign 01", sep = "")) + scale_size(range = c(2, 5))

kable(head(arrange(wip1, desc(cd1)), digits=2))
kable(head(arrange(wip1, desc(md1)), digits=2))

```

## Campaign 02

```{r, echo=FALSE,message=FALSE,warning=FALSE}
## Calculate overall Mahalonbis Distance for clicks and conversion numbers to identify outlying values.
sx2 <- cov(fitSet02.dt[Campaign == "Campaign 02" & Clicks > 0,6:7, with = FALSE])
md2 <- mahalanobis(fitSet02.dt[Campaign == "Campaign 02" & Clicks > 0,6:7, with = FALSE], colMeans(fitSet02.dt[Campaign == "Campaign 02", 6:7, with = FALSE]), sx2)

## Caculate and add Cook's Distance as per the fited model.
cd2 <- cooks.distance(
  lm(Converted.clicks ~ Clicks, data = fitSet02.dt[Campaign == "Campaign 02" & Clicks > 0], weight = Cost)
  )
wip2 <- cbind(fitSet02.dt[Campaign == "Campaign 02" & Clicks > 0,], md2, cd2)

ggplot(wip2, aes(x = log(Clicks), y = log(Converted.clicks))) + geom_point(aes(colour = md2, size = cd2)) + geom_text(aes(label = ifelse((cd2 > 4/nrow(wip2)), ngram, "")), hjust = 1, vjust = 1) + ggtitle(paste("Clicks to Converted Clicks for Campaign 02", sep = "")) + scale_size(range = c(2, 5))

kable(head(arrange(wip2, desc(cd2)), digits=2))
kable(head(arrange(wip2, desc(md2)), digits=2))

```

## Campaign 03

```{r, echo=FALSE,message=FALSE,warning=FALSE}
## Calculate overall Mahalonbis Distance for clicks and conversion numbers to identify outlying values.
sx3 <- cov(fitSet02.dt[Campaign == "Campaign 03" & Clicks > 0,6:7, with = FALSE])
md3 <- mahalanobis(fitSet02.dt[Campaign == "Campaign 03" & Clicks > 0,6:7, with = FALSE], colMeans(fitSet02.dt[Campaign == "Campaign 03", 6:7, with = FALSE]), sx1)

## Caculate and add Cook's Distance as per the fited model.
cd3 <- cooks.distance(
  lm(Converted.clicks ~ Clicks, data = fitSet02.dt[Campaign == "Campaign 03" & Clicks > 0], weight = Cost)
  )
wip3 <- cbind(fitSet02.dt[Campaign == "Campaign 03" & Clicks > 0,], md3, cd3)

ggplot(wip3, aes(x = log(Clicks), y = log(Converted.clicks))) + geom_point(aes(colour = md3, size = cd3)) + geom_text(aes(label = ifelse((cd3 > 4/nrow(wip3)), ngram, "")), hjust = 1, vjust = 1) + ggtitle(paste("Clicks to Converted Clicks for Campaign 03", sep = "")) + scale_size(range = c(2, 5))

kable(head(arrange(wip3, desc(cd3)), digits=2))
kable(head(arrange(wip3, desc(md3)), digits=2))

```

## Campaign 04

```{r, echo=FALSE,message=FALSE,warning=FALSE}
## Calculate overall Mahalonbis Distance for clicks and conversion numbers to identify outlying values.
sx4 <- cov(fitSet02.dt[Campaign == "Campaign 04" & Clicks > 0,6:7, with = FALSE])
md4 <- mahalanobis(fitSet02.dt[Campaign == "Campaign 04",6:7, with = FALSE], colMeans(fitSet02.dt[Campaign == "Campaign 04" & Clicks > 0, 6:7, with = FALSE]), sx1)

## Caculate and add Cook's Distance as per the fited model.
cd4 <- cooks.distance(
  lm(Converted.clicks ~ Clicks, data = fitSet02.dt[Campaign == "Campaign 04" & Clicks > 0], weight = Cost)
  )
wip4 <- cbind(fitSet02.dt[Campaign == "Campaign 04" & Clicks > 0,], md4, cd4)

ggplot(wip4, aes(x = log(Clicks), y = log(Converted.clicks))) + geom_point(aes(colour = md4, size = cd4)) + geom_text(aes(label = ifelse((cd4 > 4/nrow(wip4)), ngram, "")), hjust = 1, vjust = 1) + ggtitle(paste("Clicks to Converted Clicks for Campaign 04", sep = "")) + scale_size(range = c(2, 5))

kable(head(arrange(wip4, desc(cd4)), digits=2))
kable(head(arrange(wip4, desc(md4)), digits=2))

```